{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "093e937d-6ef3-4849-8ed4-f93bdb929e2b",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3857b5f-7dbe-4a16-8f85-45fbb378375f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ffb93af-3006-47c0-b2bb-c5285accf643",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define constant \n",
    "\n",
    "RNA_BASES = [[\"A\"], [\"U\"], [\"C\"], [\"G\"]]\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(RNA_BASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709273a-1827-4882-bf11-d63dc20846ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    \"\"\"\n",
    "    Parse command-line arguments.\n",
    "    Returns:\n",
    "        argparse.Namespace: An object containing the parsed command-line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Data Preprocessing Script\")\n",
    "    parser.add_argument('--input_file', required=True, help=\"Path to the input CSV file\")\n",
    "    parser.add_argument('--output_file', required=True, help=\"Path to save the processed data\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "652452fa-fda3-41a6-9a3d-a567470a8c54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(file_path) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Load data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the CSV file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - data(DataFrame): A Pandas DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    data = pd.read.csv(file_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46aa1c6-9909-4bad-82a9-5084cf98a5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_SN(data):\n",
    "    \n",
    "    \"\"\" \n",
    "    Filter the rows where \"SN_filter\" is equal to 1 \n",
    "       \n",
    "    Parameters :\n",
    "    -data(DataFrame) : A pandas Dataframe containing the input data\n",
    "    \n",
    "    Returns :\n",
    "    - cleaned_train_data(DataFrame) : A dataframe containing only the sequences that passed the SN_filter \n",
    "    \"\"\"\n",
    "    cleaned_train_data = data[data['SN_filter'] == 1]\n",
    "    return cleaned_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9a7e2b-6a5b-4709-8e13-020005c5a0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_identical_sequences(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    For identical sequences keep rows with maximum signal to noise\n",
    "    \n",
    "    Parameters:\n",
    "    -data (DataFrame) : pandas input dataframe\n",
    "    \n",
    "    Returns :\n",
    "    -filtered_df(Dataframe) : a filtered dataframe with the sequences with a maximum signal to noise\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_df = cleaned_train_data.groupby('sequence').apply(lambda x: x.loc[x['signal_to_noise'].idxmax()])\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5a0fa3-dd4b-447f-892b-eb72d50f8dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def columns_defining(cleaned_train_data) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Define the X and Y columns\n",
    "    \n",
    "    Parameters:\n",
    "    -cleaned_trained_data (Dataframe) : pandas dataframe input\n",
    "    \n",
    "    Returns :\n",
    "    -x_columns(list) : column names for X\n",
    "    -y_columns(list) : column names for Y\n",
    "    -conditional_columns(list) : continonal columns for the whole dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    x_columns = [\"sequence_id\", \"sequence\"]\n",
    "    conditional_columns = [\"experiment_type\", \"signal_to_noise\"]\n",
    "    y_columns = [colname for colname in cleared_train_data.columns if re.match(\"^reactivity_[0-9]{4}$\", colname)]\n",
    "    return x_columns, y_columns, conditional_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c914bfcc-ab9c-4b16-9679-e16d5a333bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def column_filtering(cleaned_train_data, x_columns, y_columns, conditional_columns) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Select the necessary columns in the dataframe\n",
    "    \n",
    "    Parameters :\n",
    "    -cleaned_train_data(DataFrame) : dataframe input\n",
    "    -x_columns(list): list of columns for X\n",
    "    -y_columns(list): list of columns for Y\n",
    "    -conditional_columns(list): list of conditional columns\n",
    "    \n",
    "    Returns : \n",
    "    -cleaned_train_data(DataFrame) : dataframe with X and Y columns and the conditional ones\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_train_data = cleaned_train_data[x_columns + conditional_columns + y_columns]\n",
    "    return cleaned_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe1cfb70-10dd-49e5-9b22-dc4e4f2f02bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataframe_separation(cleaned_train_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create two dataframe based on the experiment type (DMS and 2A3)\n",
    "    \n",
    "    Parameters :\n",
    "    -cleaned_train_data(DataFrame) : input dataframe to be separated in two\n",
    "    \n",
    "    Returns :\n",
    "    -df_2A3_MaP(DataFrame) : dataframe with the 2A3 results only\n",
    "    -df_DMS_MaP(DataFrame) : dataframe with the DMS results only\n",
    "    \n",
    "    \"\"\"\n",
    "    df_2A3_MaP = cleaned_train_data[cleaned_train_data['experiment_type'] == '2A3_MaP']\n",
    "    df_DMS_MaP = cleaned_train_data[cleaned_train_data['experiment_type'] == 'DMS_MaP']\n",
    "    \n",
    "    # Delete cleaned_train_data to free space memory\n",
    "    del cleaned_train_data\n",
    "    \n",
    "    return df_2A3_MaP, df_DMS_MaP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbea94ea-e7a2-4cac-a7cb-a9dd4a7b358b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataframe_concatenation(df_2A3_MaP, df_DMS_MaP) : \n",
    "    \n",
    "    \"\"\"\n",
    "    Concatenation of the experiment dataframes\n",
    "    \n",
    "    Parameters :\n",
    "    - df_2A3_MaP(DataFrame) : dataframe with the 2A3 experiment results\n",
    "    - df_DMS_MaP(DataFrame) : dataframe with the DMS experiment results\n",
    "    \n",
    "    Returns :\n",
    "    \n",
    "    -cleared_train_data(DataFrame) : concatenated dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    mask_2A3 = df_2A3_MaP[\"sequence\"].isin(df_DMS_MaP[\"sequence\"])\n",
    "    mask_DMS = df_DMS_MaP[\"sequence\"].isin(df_2A3_MaP[\"sequence\"])\n",
    "    \n",
    "    cleared_train_data = pd.concat([df_2A3_MaP[mask_2A3], df_DMS_MaP[mask_DMS]], ignore_index=True)\n",
    "    \n",
    "    return cleared_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecfe44e2-a303-4b6a-a45b-e77dd535e841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reactivaty_normalization(df_2A3_MaP, df_DMS_MaP) :\n",
    "    \"\"\"\n",
    "    Robust Z-score Normalization of reactivities \n",
    "    \n",
    "    Parameters :\n",
    "    \n",
    "    - df_2A3_MaP(DataFrame) : dataframe with the 2A3 experiment results\n",
    "    - df_DMS_MaP(DataFrame) : dataframe with the DMS experiment results\n",
    "    \n",
    "    Returns :\n",
    "    \n",
    "    - df_2A3_MaP(DataFrame) : Normalized dataframe with the 2A3 experiment results\n",
    "    - df_DMS_MaP(DataFrame) : Normalized dataframe with the DMS experiment results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate median and Median Absolute Deviation (MAD) for robust normalization\n",
    "    dms_median = np.nanmedian(df_2A3_MaP)\n",
    "    a3_median = np.nanmedian(df_DMS_MaP)\n",
    "    dms_mad = np.nanmedian(np.abs(np.array(df_DMS_MaP) - dms_median))\n",
    "    a3_mad = np.nanmedian(np.abs(np.array(df_2A3_MaP) - a3_median))\n",
    "\n",
    "    # Print the calculated values\n",
    "    print(\"dms_median:\", dms_median)\n",
    "    print(\"a3_median:\", a3_median)\n",
    "    print(\"dms_mad:\", dms_mad)\n",
    "    print(\"a3_mad:\", a3_mad)\n",
    "\n",
    "    # Apply robust z-score normalization to all DataFrames\n",
    "    for i in range(len(df_2A3_MaP)):\n",
    "        df_DMS_MaP[i]['DMS_MaP_Reactivity'] = (df_DMS_MaP[i]['DMS_MaP_Reactivity'] - dms_median) / (1.482602218505602 * dms_mad)\n",
    "     \n",
    "    for i in range(len(df_DMS_MaP)):    \n",
    "        df_2A3_MaP[i]['2A3_DMS_Reactivity'] = (df_2A3_MaP[i]['2A3_MaP_Reactivity'] - a3_median) / (1.482602218505602 * a3_mad)\n",
    "\n",
    "    \n",
    "    return df_DMS_MaP, df_2A3_MaP\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15099132-66b6-47c2-9268-0c1b4d4e4a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_reset(cleared_train_data) : #anis data\n",
    "    \"\"\"\n",
    "    Reset the index of a DataFrame and add the old index as a new column.\n",
    "\n",
    "    Parameters:\n",
    "    cleared_train_data(pd.DataFrame): The DataFrame to reset the index of.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    cleared_train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e252f1a0-34ac-462f-8125-a24bacdf1c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rm_signal_to_noise(cleared_train_data) : #roude data\n",
    "    \"\"\"\n",
    "    Drop signal to noise\n",
    "    \n",
    "    Parameters :\n",
    "    - cleared_train_data(pd.Dataframe) : Dataframe to be filtered\n",
    "    \n",
    "    Returns :\n",
    "    \n",
    "    - cleared_train_data(pd.Dataframe) : Dataframe without signal_to_noise_column\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    cleared_train_data = cleared_train_data.drop(columns=['signal_to_noise'])\n",
    "    \n",
    "    return cleared_train_data\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6fcbe9d-d2fa-4f61-9512-55ae51682f02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extraction(data) : #may need to be removed\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract RNA sequences and experiment type\n",
    "    \n",
    "    Parameters :\n",
    "    - data(Dataframe) : preloaded dataframe with the right format\n",
    "    \n",
    "    Returns:\n",
    "    - rna_sequence(Series) : a pandas series with all the sequences\n",
    "    - experiment_type(Series) : a panda series with the experiment types\n",
    "    \"\"\"\n",
    "    \n",
    "    rna_sequences = data['sequence']\n",
    "    experiment_type = data['experiment_type']\n",
    "    \n",
    "    return rna_sequence, experiment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91079ea-b1cb-43af-b925-c18224d8e396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def features_encoding(rna_sequences, experiment_type) : #may need to be removed\n",
    "    \n",
    "    \"\"\"\n",
    "    Fit and transform RNA sequences\n",
    "    \n",
    "    Parameters :\n",
    "    - rna_sequence(Series) : a pandas series with all the RNA sequences\n",
    "    - experiment_type(Series) : a panda series with the experiment types\n",
    "    \n",
    "    Returns :\n",
    "    - features(matrix) : concatenated encoded matrix\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    encoder = OneHotEncoder(sparse_output = True, dtype=np.int64)\n",
    "    rna_sequences_encoded = encoder.fit_transform(rna_sequences.values.reshape(-1,1))\n",
    "    experiment_type_encoded = pd.get_dummies(experiment_type)\n",
    "    features = hstack((rna_sequences_encoded, experiment_type_encoded))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da5a9e1-a4cd-4ffe-ac59-2c3e0b8388ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_target(cleared_train_data) : #may need to be removed\n",
    "    \n",
    "    \"\"\"\n",
    "    Extract reactivity columns as targets to use as Y\n",
    "    \n",
    "    Parameters :\n",
    "    cleared_train_data(DataFrame) : pandas dataframe with the whole data\n",
    "    \n",
    "    Returns : \n",
    "    targets(DataFrame) : dataframe with reactivity columns\n",
    "    \"\"\"\n",
    "    \n",
    "    reactivity_columns = cleared_train_data.columns[~cleared_train_data.columns.isin(['sequence','experiment_type'])]\n",
    "    targets = cleared_train_data[reactivity_columns]\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8913f3c7-5173-4a03-8724-73307c234b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_inp(X_train, X_val) : #may need to be removed\n",
    "    \"\"\"\n",
    "    Reshape the input format\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature matrix (e.g., dense matrix)\n",
    "    - X_val: Validation feature matrix (e.g., dense matrix)\n",
    "\n",
    "    Returns:\n",
    "    - X_train_reshaped: Reshaped training data with time step dimension\n",
    "    - X_val_reshaped: Reshaped validation data with time step dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert dense matrix to dense array \n",
    "    \n",
    "    X_train_dense = X_train.toarray()\n",
    "    X_val_dense = X_val.toarray()\n",
    "    \n",
    "    # Reshape input data to include the time step dimension\n",
    "    timesteps = 1  # Number of time steps (since since we have masked sequences)\n",
    "    input_dim = X_train_dense.shape[1]\n",
    "    X_train_reshaped = X_train_dense.reshape(X_train_dense.shape[0], timesteps, input_dim)\n",
    "    X_val_reshaped = X_val_dense.reshape(X_val_dense.shape[0], timesteps, input_dim)\n",
    "    \n",
    "    return X_val_reshaped, X_val_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fca7d7ea-2a25-4688-a8a8-1e43e4eea035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def onehot_from_sequence(sequence, encoder, to_add=\"0\", maxlen=457):\n",
    "    \"\"\"\n",
    "    Encoder and padder for sequences :\n",
    "    Parameters :\n",
    "    sequence (str) : sequence of nucleotides to be encoded\n",
    "    encoder () : encoder from keras\n",
    "    \n",
    "    Returns :\n",
    "    one_hot_sequence() : encoded and padded sequence\n",
    "    \"\"\"\n",
    "    proccessed_sequence = sequence.upper()\n",
    "    proccessed_sequence += to_add * (maxlen - len(sequence)) #padding sequence\n",
    "    proccessed_sequence = [[nbase] for nbase in proccessed_sequence]\n",
    "    onehot_sequence = encoder.transform(proccessed_sequence)\n",
    "    return onehot_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27cdd264-ba00-4172-999f-8b77960dbe70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padded_matrix(matrix_2d, maxlen=457):\n",
    "    \"\"\"\n",
    "    Padder for Y values\n",
    "    Parameters :\n",
    "    - matrix_2D(matrix) : Matrix to be padded\n",
    "    Returns :\n",
    "    - matrix_2S_padded(matrix) : Padded matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(matrix_2d, np.ndarray):\n",
    "        matrix_2d = np.array(matrix_2d)\n",
    "\n",
    "    n_toadd = maxlen - matrix_2d.shape[0] #number of n to add\n",
    "    padding = ((0, n_toadd), (0, 0))  # padding on axis\n",
    "    matrix_2d_padded = np.pad(matrix_2d, pad_width=padding, mode=\"constant\")\n",
    "    return matrix_2d_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b20bc14-92a3-4400-8317-d31df1009861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_XY(seq_reactivity, encoder, maxlen=457) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Encoding X and padding Y\n",
    "    Parameters :\n",
    "    dict_data(Dictionnary) : dictionnary issued from cleared data\n",
    "    encoder : encoder \n",
    "    Returns :\n",
    "    x_list(np array) : encoded sequences\n",
    "    y_list(np array) : padded reactivities\n",
    "    \"\"\"\n",
    "    \n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for i, (sequence, reactivities) in enumerate(dict_data.items()):\n",
    "        y = np.hstack([reactivities[\"2A3_MaP\"], reactivities[\"DMS_MaP\"]])\n",
    "        x_list.append(onehot_from_sequence(sequence, encoder, maxlen=maxlen))\n",
    "        y_list.append(padded_matrix(y, maxlen=maxlen))\n",
    "    return np.array(x_list), np.array(y_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8dd8139-4f27-4160-a9c2-405bf6b6b1c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_seq_reactivity(cleared_train_data, keys_name=[\"2A3_MaP\", \"DMS_MaP\"]) :\n",
    "    \"\"\"\n",
    "    Parameters (DataFrame) : input dataframe to extract reactivities from\n",
    "    - data (DataFrame) : input dataframe to extract reactivities from\n",
    "    \n",
    "    Returns :\n",
    "    seq_reactivity (Dictionnary) : reactivities for each sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    n_duplicate = sum(data.duplicated(subset=[\"sequence\", \"experiment_type\"]))\n",
    "    if n_duplicate > 0:\n",
    "        return None\n",
    "    \n",
    "    seq_reactivity = dict()\n",
    "    for seq, group in data.groupby(\"sequence\"):\n",
    "        seq_reactivity[seq] = dict()\n",
    "        for key in keys_name :\n",
    "            mask = group[\"experiment_type\"] == key\n",
    "            seq_reactivity[seq][key] = group[mask].drop(\n",
    "                labels=[\"sequence\", \"experiment_type\"], axis=1\n",
    "            ).values.reshape(-1)\n",
    "            seq_reactivity[seq][key] = np.expand_dims(seq_reactivity[seq][key], axis=1)\n",
    "\n",
    "    return seq_reactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "538b661e-4bf4-4ee0-a414-14024045bc62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def csv_export(cleared_train_data, y_columns) :\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the dataframe into a csv file\n",
    "    \n",
    "    Parameters : \n",
    "    - cleared_train_data(DataFrame) : dataframe with the whole data\n",
    "    - y_columns(list) : columns for Y data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert the y columns     \n",
    "    cleared_train_data[y_columns] = cleared_train_data[y_columns].astype(np.float32)\n",
    "    \n",
    "    #export the csv\n",
    "    csv_path = input(\"Enter your path :\") \n",
    "    cleared_train_data.to_csv(csv_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "705eb222-75fb-4374-b6dc-8cd87512664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reactivity_masking(y) : \n",
    "    \n",
    "    \"\"\"\n",
    "    Handle Na reactivity values by creating a mask\n",
    "    \n",
    "    Parameters : \n",
    "    y(DataFrame) : dataframe with reactivity columns\n",
    "    \n",
    "    Returns :\n",
    "    reactivity_mask(Boolean mask) : mask for na reactivity values\n",
    "    \"\"\"\n",
    "    \n",
    "    x_mask = x.sum(axis=2) == 0\n",
    "    reactivity_mask = ~np.isnan(y.values)\n",
    "    return reactivity_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72090c34-4546-4402-a638-b8fc4611b554",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_val_sets(features, targets, reactivity_mask): \n",
    "    \n",
    "    \"\"\"\n",
    "    Creates the Validation and the training sets\n",
    "    \n",
    "    Parameters :\n",
    "    - features(matrix) : concatenated encoded matrix\n",
    "    - targets(DataFrame) : dataframe with reactivity columns\n",
    "    - reactivity_mask(Boolean mask) : mask for na reactivity values\n",
    "    \n",
    "    Returns :\n",
    "    \n",
    "    - X_train\n",
    "    - X_val\n",
    "    - y_train\n",
    "    - y_val\n",
    "    - mask_train\n",
    "    - mask_val\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train, X_val, y_train, y_val, mask_train, mask_val = train_test_split(\n",
    "        features, targets, reactivity_mask, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, mask_train, mask_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ceadb9-2284-4e37-b49b-1ec95825bcc8",
   "metadata": {},
   "source": [
    "ANIS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a356ed4-da38-49c8-9957-83dff960899e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_columns(data) : \n",
    "    \"\"\"\n",
    "    Remove unnecessary columns\n",
    "    Parameters :\n",
    "    -data(dataframe) : dataset of the data\n",
    "    Returns :\n",
    "    -cleaned_train_data(dataframe) : cleaned dataset without useless columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of columns to remove\n",
    "    columns_to_remove = [\"sequence_id\", \"dataset_name\", \"reads\", \"SN_filter\"]\n",
    "\n",
    "    # Find columns containing \"reactivity_error\" in their names\n",
    "    reactivity_error_columns = [col for col in data.columns if \"reactivity_error\" in col]\n",
    "\n",
    "    # Combine the columns to remove\n",
    "    columns_to_remove.extend(reactivity_error_columns)\n",
    "\n",
    "    # Drop the specified columns from the DataFrame\n",
    "    cleaned_train_data = data.drop(columns=columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a55599b-26f8-44e4-89f9-d5d81523408d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pandas_dataset_converting(data) :\n",
    "    \"\"\"\n",
    "    Parameters :\n",
    "    - data(dataframe) : original_dataset\n",
    "    Returns : \n",
    "    - computed_data (dataframe) : pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    computed_data = data.compute()\n",
    "    return computed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e51e3315-4642-46de-af3c-697516bff79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduce memory usage \n",
    "    Parameters :\n",
    "    -df(Dataframe) : dataframe with important memory\n",
    "    Returns :\n",
    "    -df(Dataframe) : dataframe with reduced dimensions\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09acd5bf-8f61-4865-9239-f9be0830cdf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to process the entire DataFrame\n",
    "def process_dataframe(df):\n",
    "    \"\"\"\n",
    "    Process the whole dataframe \n",
    "    Parameters :\n",
    "    -df(Dataframe) : Dataframe with important data\n",
    "    Return :\n",
    "    -array_list(np.array) : input for the model\n",
    "    \"\"\"\n",
    "    \n",
    "    result_list = []\n",
    "    unique_sequences = df['sequence'].unique() #filter unique sequences\n",
    "\n",
    "    # Create empty lists to store values for robust normalization\n",
    "    dms_values = []\n",
    "    a3_values = []\n",
    "\n",
    "    for sequence in unique_sequences:\n",
    "        sequence_data = df[df['sequence'] == sequence]\n",
    "        dms_map_row = sequence_data[sequence_data['experiment_type'] == 'DMS_MaP']\n",
    "        a3_map_row = sequence_data[sequence_data['experiment_type'] == '2A3_MaP']\n",
    "\n",
    "        tensor_rows = []\n",
    "        for i in range(len(sequence)):\n",
    "            dms_value = dms_map_row[f'reactivity_{i+1:04d}'].iloc[0] if not dms_map_row.empty else np.nan\n",
    "            a3_value = a3_map_row[f'reactivity_{i+1:04d}'].iloc[0] if not a3_map_row.empty else np.nan\n",
    "            nucleotide = sequence[i]\n",
    "            tensor_row = [nucleotide, dms_value, a3_value]\n",
    "            tensor_rows.append(tensor_row)\n",
    "\n",
    "            # Append values for robust normalization, only if they are not NaN\n",
    "            if not np.isnan(dms_value):\n",
    "                dms_values.append(dms_value)\n",
    "            if not np.isnan(a3_value):\n",
    "                a3_values.append(a3_value)\n",
    "\n",
    "        # Create a DataFrame from the tensor_rows\n",
    "        tensor_df = pd.DataFrame(tensor_rows, columns=['Nucleotide', 'DMS_MaP_Reactivity', '2A3_MaP_Reactivity'])\n",
    "\n",
    "        # Perform one-hot encoding for the 'Nucleotide' column\n",
    "        nucleotide_encoded = pd.get_dummies(tensor_df['Nucleotide'], prefix='Nucleotide')\n",
    "\n",
    "        # Drop the original 'Nucleotide' column\n",
    "        tensor_df.drop(columns=['Nucleotide'], inplace=True)\n",
    "\n",
    "        # Concatenate the one-hot encoded columns with the original DataFrame\n",
    "        tensor_df = pd.concat([nucleotide_encoded, tensor_df], axis=1)\n",
    "\n",
    "        result_list.append(tensor_df)\n",
    "\n",
    "    # Calculate median and Median Absolute Deviation (MAD) for robust normalization\n",
    "    dms_median = np.nanmedian(dms_values)\n",
    "    a3_median = np.nanmedian(a3_values)\n",
    "    dms_mad = np.nanmedian(np.abs(np.array(dms_values) - dms_median))\n",
    "    a3_mad = np.nanmedian(np.abs(np.array(a3_values) - a3_median))\n",
    "\n",
    "    # Print the calculated values\n",
    "    print(\"dms_median:\", dms_median)\n",
    "    print(\"a3_median:\", a3_median)\n",
    "    print(\"dms_mad:\", dms_mad)\n",
    "    print(\"a3_mad:\", a3_mad)\n",
    "\n",
    "    # Apply robust z-score normalization to all DataFrames\n",
    "    for i in range(len(result_list)):\n",
    "        result_list[i]['DMS_MaP_Reactivity'] = (result_list[i]['DMS_MaP_Reactivity'] - dms_median) / (1.482602218505602 * dms_mad)\n",
    "        result_list[i]['2A3_MaP_Reactivity'] = (result_list[i]['2A3_MaP_Reactivity'] - a3_median) / (1.482602218505602 * a3_mad)\n",
    "\n",
    "    # Convert each data frame into a NumPy array\n",
    "    array_list = [df.to_records(index=False) for df in result_list]\n",
    "\n",
    "    return array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e47d08-ea1d-4340-848e-61983caa393f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ddbcb-7450-4615-b71a-7d35fd2ef290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
