{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byH5N1HEopQ-"
   },
   "source": [
    "# **Load train data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZomOEm_SyL31"
   },
   "source": [
    "Load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/SN_filtered_train.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5KhTQ-qwfbG"
   },
   "source": [
    "# **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwgn5zFGws7F"
   },
   "source": [
    "To be completed: add analysis of shape of the data, pycharts, boxplots, pehaps PCA, univariate analysis, multivariate analysis..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "executionInfo": {
     "elapsed": 8093,
     "status": "ok",
     "timestamp": 1696108835445,
     "user": {
      "displayName": "Ghosty Science",
      "userId": "16741200961527128262"
     },
     "user_tz": -120
    },
    "id": "pFKZeActGokI",
    "outputId": "7410bd77-a330-45c1-eca4-7a3a276074f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>experiment_type</th>\n",
       "      <th>dataset_name</th>\n",
       "      <th>reads</th>\n",
       "      <th>signal_to_noise</th>\n",
       "      <th>SN_filter</th>\n",
       "      <th>reactivity_0001</th>\n",
       "      <th>reactivity_0002</th>\n",
       "      <th>reactivity_0003</th>\n",
       "      <th>...</th>\n",
       "      <th>reactivity_error_0197</th>\n",
       "      <th>reactivity_error_0198</th>\n",
       "      <th>reactivity_error_0199</th>\n",
       "      <th>reactivity_error_0200</th>\n",
       "      <th>reactivity_error_0201</th>\n",
       "      <th>reactivity_error_0202</th>\n",
       "      <th>reactivity_error_0203</th>\n",
       "      <th>reactivity_error_0204</th>\n",
       "      <th>reactivity_error_0205</th>\n",
       "      <th>reactivity_error_0206</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51e61fbde94d</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUC...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>5326.0</td>\n",
       "      <td>1.933</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25ce8d5109cd</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUC...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>4647.0</td>\n",
       "      <td>2.347</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07dcfb6d1965</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUC...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>102843.0</td>\n",
       "      <td>11.824</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e561cc042a4c</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUC...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>7665.0</td>\n",
       "      <td>3.519</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aa948762535f</td>\n",
       "      <td>GGGAACGACUCGAGUAGAGUCGAAAAACGCUGAUAUGGAUUUACUC...</td>\n",
       "      <td>2A3_MaP</td>\n",
       "      <td>15k_2A3</td>\n",
       "      <td>14018.0</td>\n",
       "      <td>3.219</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 419 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sequence_id                                           sequence  \\\n",
       "0  51e61fbde94d  GGGAACGACUCGAGUAGAGUCGAAAAACAUUGAUAUGGAUUUACUC...   \n",
       "1  25ce8d5109cd  GGGAACGACUCGAGUAGAGUCGAAAAACCUUGAUAUGGAUUUACUC...   \n",
       "2  07dcfb6d1965  GGGAACGACUCGAGUAGAGUCGAAAAACUUUGAUAUGGAUUUACUC...   \n",
       "3  e561cc042a4c  GGGAACGACUCGAGUAGAGUCGAAAAACGAUGAUAUGGAUUUACUC...   \n",
       "4  aa948762535f  GGGAACGACUCGAGUAGAGUCGAAAAACGCUGAUAUGGAUUUACUC...   \n",
       "\n",
       "  experiment_type dataset_name     reads  signal_to_noise  SN_filter  \\\n",
       "0         2A3_MaP      15k_2A3    5326.0            1.933          1   \n",
       "1         2A3_MaP      15k_2A3    4647.0            2.347          1   \n",
       "2         2A3_MaP      15k_2A3  102843.0           11.824          1   \n",
       "3         2A3_MaP      15k_2A3    7665.0            3.519          1   \n",
       "4         2A3_MaP      15k_2A3   14018.0            3.219          1   \n",
       "\n",
       "   reactivity_0001  reactivity_0002  reactivity_0003  ...  \\\n",
       "0              NaN              NaN              NaN  ...   \n",
       "1              NaN              NaN              NaN  ...   \n",
       "2              NaN              NaN              NaN  ...   \n",
       "3              NaN              NaN              NaN  ...   \n",
       "4              NaN              NaN              NaN  ...   \n",
       "\n",
       "   reactivity_error_0197  reactivity_error_0198  reactivity_error_0199  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0200  reactivity_error_0201  reactivity_error_0202  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0203  reactivity_error_0204  reactivity_error_0205  \\\n",
       "0                    NaN                    NaN                    NaN   \n",
       "1                    NaN                    NaN                    NaN   \n",
       "2                    NaN                    NaN                    NaN   \n",
       "3                    NaN                    NaN                    NaN   \n",
       "4                    NaN                    NaN                    NaN   \n",
       "\n",
       "   reactivity_error_0206  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  \n",
       "\n",
       "[5 rows x 419 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 5 rows of the DataFrame\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDm3n7BKw_oZ"
   },
   "source": [
    "# **Feature engeneering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8kB0ID6Z9De"
   },
   "source": [
    "Remove \"sequence_id\", \"dataset_name\", \"reads\", \"SN_filter\" and \"reactivity_error\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to keep rows with maximum signal_to_noise within identical sequences\n",
    "def filter_identical_sequences(df):\n",
    "    # Group by 'sequence' and keep the row with max 'signal_to_noise'\n",
    "    filtered_df = df.groupby('sequence').apply(lambda x: x.loc[x['signal_to_noise'].idxmax()])\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "DNji9XrsIFEv"
   },
   "outputs": [],
   "source": [
    "# Get columns name for X, and Y\n",
    "x_columns = [\"sequence\"]\n",
    "conditional_columns = [\"experiment_type\", \"signal_to_noise\"]\n",
    "y_columns = [colname for colname in train_data.columns if re.match(\"^reactivity_[0-9]{4}$\", colname)]\n",
    "\n",
    "# Keep the necessary columns from the DataFrame\n",
    "cleaned_train_data = train_data[x_columns + conditional_columns + y_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqDa3YTXBZQ3"
   },
   "source": [
    "For each group of identical sequences, keep only the sequence with the highest signal to noise value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "7zlSxgrf4_MP"
   },
   "outputs": [],
   "source": [
    "# Create two separate DataFrames based on \"experiment_type\"\n",
    "df_2A3_MaP = cleaned_train_data[cleaned_train_data['experiment_type'] == '2A3_MaP']\n",
    "df_DMS_MaP = cleaned_train_data[cleaned_train_data['experiment_type'] == 'DMS_MaP']\n",
    "\n",
    "# Delete cleaned_train_data to free space memory\n",
    "del cleaned_train_data\n",
    "\n",
    "df_2A3_MaP = filter_identical_sequences(df_2A3_MaP)  # Filter df_2A3_MaP\n",
    "df_DMS_MaP = filter_identical_sequences(df_DMS_MaP)  # Filter df_DMS_MaP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two data frames\n",
    "mask_2A3 = df_2A3_MaP[\"sequence\"].isin(df_DMS_MaP[\"sequence\"])\n",
    "mask_DMS = df_DMS_MaP[\"sequence\"].isin(df_2A3_MaP[\"sequence\"])\n",
    "\n",
    "cleared_train_data = pd.concat([df_2A3_MaP[mask_2A3], df_DMS_MaP[mask_DMS]], ignore_index=True)\n",
    "cleared_train_data.drop(columns=['signal_to_noise'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PsUS4aNI5LP"
   },
   "source": [
    "Save the cleared train data into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns type\n",
    "cleared_train_data[y_columns] = cleared_train_data[y_columns].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "RpY4-pTPI_bQ"
   },
   "outputs": [],
   "source": [
    "# Save cleared_train_data as a CSV file\n",
    "csv_path = './data/cleared_train_data.csv'\n",
    "cleared_train_data.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqDict(data, by=\"experiment_type\", keys_name=[\"2A3_MaP\", \"DMS_MaP\"]):\n",
    "    n_duplicate = sum(data.duplicated(subset=[\"sequence\", by]))\n",
    "    if n_duplicate > 0:\n",
    "        return None\n",
    "\n",
    "    seq_reactivity = dict()\n",
    "    for seq, group in data.groupby(\"sequence\"):\n",
    "        seq_reactivity[seq] = dict()\n",
    "        for key in keys_name:\n",
    "            mask = group[by] == key\n",
    "            seq_reactivity[seq][key] = group[mask].drop(\n",
    "                labels=[\"sequence\", \"experiment_type\"], axis=1\n",
    "            ).values\n",
    "\n",
    "    return seq_reactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GGGAACGACUCGAGUAGAGUCGAAAAAAAAAAAAAAGAUAUGGACAUCGUGUUGUGAUGACCCUACCCACCUACAUCGGAAACGAUGUAGAGGGCAAGCAACAGAUGUCCUAAGUCAAAAAAAAAACAAGUACCUUAUGUUCGCAUAAGGUACUUGAAAAGAAACAACAACAACAAC': {'2A3_MaP': array([[   nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,  0.   , -0.121,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "           2.379,  4.177,  0.854,  0.726,  0.477,  1.121,  1.541,  1.037,\n",
       "           0.7  ,  0.019, -0.121,  0.   ,  0.14 ,  0.   , -0.121,  0.56 ,\n",
       "           3.222,  0.56 , -0.345,  1.034,  0.162,  0.14 ,  0.   ,  0.057,\n",
       "           1.401,  2.381,  0.159,  0.981, -0.428, -0.121,  0.   ,  1.158,\n",
       "           1.401,  0.28 ,  0.178,  0.   ,  0.178,  0.841,  0.   ,  0.178,\n",
       "          -0.121,  0.14 ,  0.   ,  0.   , -0.242,  0.42 ,  0.   ,  0.102,\n",
       "           0.757,  0.   ,  0.14 , -0.121, -0.121,  0.   ,  0.14 ,  0.   ,\n",
       "          -0.121,  0.757,  0.56 ,  0.859,  0.019,  0.318,  0.28 ,  1.401,\n",
       "          -0.121,  0.28 ,  0.   ,  0.   ,  0.   , -0.121,  0.   ,  0.14 ,\n",
       "           0.019, -0.121, -0.121,  0.038,  0.   ,  0.   ,  0.   ,  0.841,\n",
       "           0.14 ,  0.   , -0.065,  0.655,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "           0.17 ,  0.   ,  0.159,  2.526,  0.   ,  0.17 ,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "             nan,    nan,    nan,    nan,    nan,    nan]]),\n",
       "  'DMS_MaP': array([[       nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan, -1.360e-01,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "           0.000e+00,  1.350e-01, -2.980e-01,  4.380e-01,  1.368e+00,\n",
       "           9.450e-01,  2.000e-03,  2.050e-01, -3.710e-01,  2.085e+00,\n",
       "           0.000e+00, -3.400e-02, -3.400e-02,  1.030e-01,  0.000e+00,\n",
       "           0.000e+00,  0.000e+00,  0.000e+00,  3.420e-01, -3.300e-02,\n",
       "           4.100e-01,  2.050e-01,  0.000e+00,  5.480e-01, -6.800e-02,\n",
       "           0.000e+00,  9.230e-01,  0.000e+00,  0.000e+00,  1.025e+00,\n",
       "          -1.330e-01, -6.800e-02,  0.000e+00, -3.300e-02,  8.200e-01,\n",
       "           2.050e-01,  2.400e-01,  1.030e-01,  4.100e-01,  2.665e+00,\n",
       "           1.030e-01, -2.030e-01,  0.000e+00,  0.000e+00,  3.500e-02,\n",
       "          -1.350e-01,  1.030e-01,  5.130e-01,  0.000e+00,  6.490e-01,\n",
       "           4.680e-01,  8.660e-01, -6.800e-02,  4.100e-01,  2.050e-01,\n",
       "          -6.800e-02,  3.500e-02, -6.800e-02,  1.030e-01,  2.000e-03,\n",
       "           6.150e-01,  3.500e-02, -1.350e-01,  3.500e-02,  1.720e-01,\n",
       "           6.930e-01,  1.222e+00, -1.350e-01, -3.300e-02,  2.050e-01,\n",
       "           0.000e+00, -6.800e-02,  2.050e-01,  0.000e+00,  1.370e-01,\n",
       "           0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,  0.000e+00,\n",
       "           0.000e+00,  2.050e-01,  3.080e-01,  0.000e+00,  3.080e-01,\n",
       "           3.420e-01,  2.170e-01,  1.490e-01,  0.000e+00,  0.000e+00,\n",
       "           0.000e+00,  0.000e+00,  6.510e-01,  1.085e+00,  1.736e+00,\n",
       "           1.666e+00,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan,        nan,        nan,        nan,        nan,\n",
       "                 nan]])}}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqDict(cleared_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzQVMwJp-amx"
   },
   "source": [
    "# **Model**\n",
    "(To be corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZT7N2KAJfFO"
   },
   "source": [
    "Load cleared train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "477pmJTr_Uoo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path of the CSV file\n",
    "csv_path = '/content/drive/My Drive/M2BI_DRIVE/cleared_train_data.csv'\n",
    "\n",
    "# Load the CSV file as a Dask DataFrame\n",
    "cleared_train_data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9PJMvm0KKkS"
   },
   "source": [
    "Define features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-EtZee-KJ2j"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Masking, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Extract RNA sequences and experiment_type\n",
    "rna_sequences = cleared_train_data['sequence']\n",
    "experiment_type = cleared_train_data['experiment_type']\n",
    "\n",
    "# Initialize the OneHotEncoder with sparse_output=True and dtype=np.int64\n",
    "encoder = OneHotEncoder(sparse_output=True, dtype=np.int64)  # Fix the warning here\n",
    "# Fit and transform RNA sequences\n",
    "rna_sequences_encoded = encoder.fit_transform(rna_sequences.values.reshape(-1, 1))\n",
    "\n",
    "# One-hot encode experiment_type using pandas\n",
    "experiment_type_encoded = pd.get_dummies(experiment_type)\n",
    "\n",
    "# Combine one-hot encoded features (RNA sequences and experiment_type)\n",
    "# Use hstack to concatenate sparse matrices\n",
    "features = hstack((rna_sequences_encoded, experiment_type_encoded))\n",
    "\n",
    "# Extract reactivity columns as targets (excluding 'sequence' and 'experiment_type')\n",
    "reactivity_columns = cleared_train_data.columns[~cleared_train_data.columns.isin(['sequence', 'experiment_type'])]\n",
    "targets = cleared_train_data[reactivity_columns]\n",
    "\n",
    "# Handle NaN values by creating a mask\n",
    "reactivity_mask = ~np.isnan(targets.values)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val, mask_train, mask_val = train_test_split(\n",
    "    features, targets, reactivity_mask, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert sparse matrices to dense arrays\n",
    "X_train_dense = X_train.toarray()\n",
    "X_val_dense = X_val.toarray()\n",
    "\n",
    "# Reshape input data to include the time step dimension\n",
    "timesteps = 1  # Number of time steps (since since we have masked sequences)\n",
    "input_dim = X_train_dense.shape[1]\n",
    "X_train_reshaped = X_train_dense.reshape(X_train_dense.shape[0], timesteps, input_dim)\n",
    "X_val_reshaped = X_val_dense.reshape(X_val_dense.shape[0], timesteps, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgJnPRBiKVSK"
   },
   "source": [
    "RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3995,
     "status": "ok",
     "timestamp": 1696118360652,
     "user": {
      "displayName": "Ghosty Science",
      "userId": "16741200961527128262"
     },
     "user_tz": -120
    },
    "id": "jDxMZJXLKWtD",
    "outputId": "4185a633-9d7f-4526-d055-598f0e8939d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 1, 242731)         0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                62155776  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 206)               13390     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62169166 (237.16 MB)\n",
      "Trainable params: 62169166 (237.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define RNN model with Masking layer\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0.0, input_shape=(timesteps, input_dim)))  # Masking layer to handle NaN values\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(y_train.shape[1], activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model (adjust the loss function and optimizer as needed)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOvFzDh1gaJa"
   },
   "source": [
    "Train the model using \"mini-batch\" training loop to avoid RAM issu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TBLcl3PVf9W"
   },
   "outputs": [],
   "source": [
    "# Define mini-batch size and number of epochs\n",
    "mini_batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Get the number of mini-batches\n",
    "num_mini_batches = X_train.shape[0] // mini_batch_size\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Training loop with mini-batch processing\n",
    "@tf.function  # Decorate the entire training loop\n",
    "def train_step(mini_batch_X, mini_batch_y, mini_batch_mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(mini_batch_X, training=True)\n",
    "        loss = tf.keras.losses.mean_squared_error(mini_batch_y, predictions)\n",
    "        mini_batch_mask = tf.cast(mini_batch_mask, dtype=tf.float32)\n",
    "        weighted_loss = loss * tf.reduce_mean(mini_batch_mask)\n",
    "\n",
    "    gradients = tape.gradient(weighted_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    for batch_idx in range(num_mini_batches):\n",
    "        start_idx = batch_idx * mini_batch_size\n",
    "        end_idx = (batch_idx + 1) * mini_batch_size\n",
    "\n",
    "        # Extract mini-batch data\n",
    "        mini_batch_X = X_train_reshaped[start_idx:end_idx]\n",
    "        mini_batch_y = y_train[start_idx:end_idx]\n",
    "        mini_batch_mask = mask_train[start_idx:end_idx]\n",
    "\n",
    "        # Train on the mini-batch\n",
    "        loss = train_step(mini_batch_X, mini_batch_y, mini_batch_mask)\n",
    "\n",
    "    # Evaluate on validation data after each epoch\n",
    "    val_loss = model.evaluate(X_val_reshaped, y_val, verbose=0)\n",
    "    print(f'Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeaV2Er3_XEe"
   },
   "source": [
    "# **Load test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyeZ3NFx_cXW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N16LwvGQ_cov"
   },
   "source": [
    "# **Check efficiency of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5g3Fg3cc_s9-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6pvbufG_g3Y"
   },
   "source": [
    "# **Save submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSdj9uMq_tf4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv5l23F3_lS5"
   },
   "source": [
    "# **Plot RNA structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52s39bl1_uOQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPuzuUz8kVToq9n1Bq0a7oY",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
